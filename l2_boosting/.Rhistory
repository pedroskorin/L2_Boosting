#L2 BOOSTING ALGORITHM
#
#Implementation of the L2 Boosting algorithm
#
#theoretical source: Jing Zang,
#
#
'''
dsadsa
'''
#
#L2 BOOSTING ALGORITHM
#
#Implementation of the L2 Boosting algorithm
#
#theoretical source: Jing Zang,
#
#
'''
dsadsa
'''
#L2 BOOSTING ALGORITHM
#
#Implementation of the L2 Boosting algorithm
#
#theoretical source: Jing Zang,
#
#
'''
dsadsa
'''
#L2 BOOSTING ALGORITHM
#
#Implementation of the L2 Boosting algorithm
#
#theoretical source: Jing Zang,
#
#
'''
dsadsa
'''
#L2 BOOSTING ALGORITHM
#
#Implementation of the L2 Boosting algorithm
#
#theoretical source: Jing Zang,
#
#
""
dsadsa
teste = 2
""
teste
""
"excluded
block"
teste
data = read.csv("https://raw.githubusercontent.com/pedroskorin/L2_Boosting/master/l2_boosting/dados.csv"[-1])
data = read.csv("https://raw.githubusercontent.com/pedroskorin/L2_Boosting/master/l2_boosting/dados.csv")[-1]
data
X = data[,-1]
X
X = data[,-10]
X$
fm = mean(Y)
Y = data$Y
X = data[,-10]
m = 0
fm = mean(Y)
ut = Y - fm
print(i)
for (i in X){
print(i)
}
print(residuals.lm(ut ~ i))
for (i in X){
print(residuals.lm(ut ~ i))
}
for (i in X){
lm = lm(ut ~ i)
}
lm
resid(lm)
sum(resid(lm))
for (i in X){
lm[,i] = lm(ut ~ i)
}
for (i in X){
lm[,i] = sum(resid(lm(ut ~ i)))
}
# Getting data from github without the first column
data = read.csv("https://raw.githubusercontent.com/pedroskorin/L2_Boosting/master/l2_boosting/dados.csv")[,-1]
source('~/GitHub/L2_Boosting/l2_boosting/main.R', echo=TRUE)
# Getting data from github without the first column
data = read.csv("https://raw.githubusercontent.com/pedroskorin/L2_Boosting/master/l2_boosting/dados.csv")[,-1]
# Selecting data
Y = data$Y # Response variable
X = data[,-ncol(data)] # Predictors variables
L2_boost = function(Y,X,M,v=0.1){
# Defining variables
ft = mean(Y) # First appearence of function ft
m = 0 # m index
u = 0 # error vector (Y-ft)
b = 0 # optimum coefficient for each predictor
teta = c() # vector of optimum coefficient for all possible predictors
sum_squared_resid = 0 # SSR for regression of ut for each predictor
SSR = c() # list of all SSR
g = c() # vector of selected predictor * optimum coefficient
g_optimum = list() # list of all g's
index = 0 # index of selected predictor in teta's vector
X_optimum = c() # Vector of selected predictor
Matrix = c() # Matrix of all selected predictors*coefficient*v
f_optimum = c() # final prediction vector
while (m <= M) { # Iterate M times
u = Y - ft # Calculating error in m
# calculating optimum coefficients
teta = c()
for (i in X){
b = solve(t(i)%*%i)%*%t(i)%*%Y
teta = c(teta, b)
}
# calculating SSR
SSR = c()
for (i in 1:length(teta)){
sum_squared_resid = sum((u - teta[i]*X[,i])^2)
SSR = c(SSR, sum_squared_resid)
}
# Minimizing SSR
index = which.min(SSR)
X_optimum = X[,index]
# Calculating g (vector of selected predictor * optimum coefficient)
g = teta[index]*X_optimum
# Updating ft
ft = ft + v*g
# Iterating
m = m + 1
g_optimum[[m]] <- g
}
# Matrix of all g vectors
Matrix = do.call(cbind, g_optimum)
# Final function
f_optimum = mean(Y) + ft
# MSE of L2_boost
print(sum((f_optimum - Y)^2)/nrow(X))
}
L2_boost(Y,X, M = 100)
L2_boost(Y,X, M = 1000)
L2_boost(Y,X, M = 10000)
seq(1,1000,10)
seq(0,1000,10)
teste = c()
for (i in seq(1,1000,100))) {
print(i)
l = L2_boost(Y,X,i)
teste = c(teste, l)
}
teste = c()
for (i in seq(1,1000,100)) {
print(i)
l = L2_boost(Y,X,i)
teste = c(teste, l)
}
plot(seq(1,1000,100), teste)
teste = c()
for (i in seq(1,1000,10)) {
print(i)
l = L2_boost(Y,X,i)
teste = c(teste, l)
}
plot(seq(1,1000,100), teste)
plot(seq(1,1000,10), teste)
teste = c()
for (i in seq(1,2000,10)) {
print(i)
l = L2_boost(Y,X,i)
teste = c(teste, l)
}
plot(seq(1,2000,10), teste)
teste = c()
for (i in seq(1,10000,100)) {
a = Sys.time()
print(i)
l = L2_boost(Y,X,i)
teste = c(teste, l)
print(Sys.time() - a)
}
plot(seq(1,2000,10), teste)
plot(seq(1,10000,100), teste)
teste = c()
for (i in seq(1,100,1)) {
a = Sys.time()
print(i)
l = L2_boost(Y,X,i)
teste = c(teste, l)
print(Sys.time() - a)
}
plot(seq(1,100,1), teste)
plot(seq(1,100,1), teste, type = "line")
teste = c()
teste = c()
for (i in seq(1,200,1)) {
a = Sys.time()
print(i)
l = L2_boost(Y,X,i)
teste = c(teste, l)
print(Sys.time() - a)
}
plot(seq(1,200,1), teste, type = "l")
teste = c()
for (i in seq(1,200,2)) {
a = Sys.time()
print(i)
l = L2_boost(Y,X,i)
teste = c(teste, l)
print(Sys.time() - a)
}
teste = c()
for (i in seq(1,400,2)) {
a = Sys.time()
print(i)
l = L2_boost(Y,X,i)
teste = c(teste, l)
print(Sys.time() - a)
}
plot(seq(1,400,2), teste, type = "l")
plot(seq(1,400,2), teste, type = "l", xlab = "m", ylab = "MSE")
# Getting data from github without the first column
data = read.csv("https://raw.githubusercontent.com/pedroskorin/L2_Boosting/master/l2_boosting/dados.csv")[,-1]
# Selecting data
Y = data$Y # Response variable
X = data[,-ncol(data)] # Predictors variables
L2_boost = function(Y,X,M,v=0.1){
# Defining variables
ft = mean(Y) # First appearence of function ft
m = 0 # m index
u = 0 # error vector (Y-ft)
b = 0 # optimum coefficient for each predictor
teta = c() # vector of optimum coefficient for all possible predictors
sum_squared_resid = 0 # SSR for regression of ut for each predictor
SSR = c() # list of all SSR
g = c() # vector of selected predictor * optimum coefficient
g_optimum = list() # list of all g's
index = 0 # index of selected predictor in teta's vector
X_optimum = c() # Vector of selected predictor
Matrix = c() # Matrix of all selected predictors*coefficient*v
f_optimum = c() # final prediction vector
choosed_predictors = rep(0,ncol(X)) # controling choosed predictors
names(choosed_predictors) = names(X)
while (m < M) { # Iterate M times
u = Y - ft # Calculating error in m
# calculating optimum coefficients
teta = c()
for (i in X){
b = solve(t(i)%*%i)%*%t(i)%*%Y
teta = c(teta, b)
}
# calculating SSR
SSR = c()
for (i in 1:length(teta)){
sum_squared_resid = sum((u - teta[i]*X[,i])^2)
SSR = c(SSR, sum_squared_resid)
}
# Minimizing SSR
index = which.min(SSR)
X_optimum = X[,index]
# Calculating g (vector of selected predictor * optimum coefficient)
g = teta[index]*X_optimum
choosed_predictors[index] = choosed_predictors[index] + 1
# Updating ft
ft = ft + v*g
# Iterating
m = m + 1
g_optimum[[m]] <- g
}
# Matrix of all g vectors
Matrix = do.call(cbind, g_optimum)
# Final function
f_optimum = mean(Y) + ft
# MSE of L2_boost
print(sum((f_optimum - Y)^2)/nrow(X))
print(choosed_predictors)
}
L2_boost(Y,X,10)
L2_boost(Y,X,20)
L2_boost(Y,X,20)
#Example
L2_boost(Y,X,20)
#Example
L2_boost(Y,X,100)
L2_boost = function(Y,X,M,v=0.1){
# Defining variables
ft = mean(Y) # First appearence of function ft
m = 0 # m index
u = 0 # error vector (Y-ft)
b = 0 # optimum coefficient for each predictor
teta = c() # vector of optimum coefficient for all possible predictors
sum_squared_resid = 0 # SSR for regression of ut for each predictor
SSR = c() # list of all SSR
g = c() # vector of selected predictor * optimum coefficient
g_optimum = list() # list of all g's
index = 0 # index of selected predictor in teta's vector
X_optimum = c() # Vector of selected predictor
Matrix = c() # Matrix of all selected predictors*coefficient*v
f_optimum = c() # final prediction vector
choosed_predictors = rep(0,ncol(X)) # controling choosed predictors
names(choosed_predictors) = names(X)
while (m < M) { # Iterate M times
u = Y - ft # Calculating error in m
# calculating optimum coefficients
teta = c()
for (i in X){
b = solve(t(i)%*%i)%*%t(i)%*%Y
teta = c(teta, b)
}
# calculating SSR
SSR = c()
for (i in 1:length(teta)){
sum_squared_resid = sum((u - teta[i]*X[,i])^2)
SSR = c(SSR, sum_squared_resid)
}
# Minimizing SSR
index = which.min(SSR)
X_optimum = X[,index]
# Calculating g (vector of selected predictor * optimum coefficient)
g = teta[index]*X_optimum
choosed_predictors[index] = choosed_predictors[index] + 1
# Updating ft
ft = ft + v*g
# Iterating
m = m + 1
g_optimum[[m]] <- g
}
# Matrix of all g vectors
Matrix = do.call(cbind, g_optimum)
# Final function
f_optimum =ft
# MSE of L2_boost
print(sum((f_optimum - Y)^2)/nrow(X))
print(choosed_predictors)
}
#Example
L2_boost(Y,X,20)
#Example
L2_boost(Y,X,100)
